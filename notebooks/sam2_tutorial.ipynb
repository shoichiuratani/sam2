{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 動画内物体検出・追跡 チュートリアル\n",
    "\n",
    "このノートブックでは、SAM2を使用した動画内物体検出・追跡の基本的な使用方法を学びます。\n",
    "\n",
    "## 目次\n",
    "1. [環境準備](#環境準備)\n",
    "2. [動画フレーム分割](#動画フレーム分割)\n",
    "3. [基本的な物体検出](#基本的な物体検出)\n",
    "4. [完全な動画追跡](#完全な動画追跟)\n",
    "5. [結果の分析](#結果の分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境準備\n",
    "\n",
    "必要なライブラリをインポートし、SAM2の準備を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# プロジェクトルートを追加\n",
    "project_root = os.path.dirname(os.getcwd()) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from src.sam2_utils import (\n",
    "    show_mask, show_points, get_frame_names, \n",
    "    load_sam2_predictor, display_frame_with_points\n",
    ")\n",
    "from src.sam2_video_tracker import SAM2VideoTracker\n",
    "\n",
    "print(f\"プロジェクトルート: {project_root}\")\n",
    "print(f\"PyTorch バージョン: {torch.__version__}\")\n",
    "print(f\"使用デバイス: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画フレーム分割\n",
    "\n",
    "まず、動画をJPEGフレームに分割する方法を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画フレーム分割の例\n",
    "# 実際の動画ファイルがある場合のコード例\n",
    "\n",
    "# from scripts.video_to_frames import video_to_frames\n",
    "# \n",
    "# input_video = \"path/to/your/video.mp4\"\n",
    "# output_dir = \"input/sample_frames\"\n",
    "# \n",
    "# if os.path.exists(input_video):\n",
    "#     frame_count = video_to_frames(input_video, output_dir, quality=95)\n",
    "#     print(f\"フレーム分割完了: {frame_count}フレーム\")\n",
    "# else:\n",
    "#     print(\"動画ファイルが見つかりません\")\n",
    "\n",
    "print(\"動画フレーム分割のコマンド例:\")\n",
    "print(\"python scripts/video_to_frames.py input.mp4 input/dog_images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フレームの確認\n",
    "\n",
    "分割されたフレームを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレームディレクトリを探す\n",
    "sample_dirs = [\n",
    "    \"input/dog_images\",\n",
    "    \"input/sample_frames\", \n",
    "    \"input/frames\"\n",
    "]\n",
    "\n",
    "video_dir = None\n",
    "for dir_path in sample_dirs:\n",
    "    full_path = os.path.join(project_root, dir_path)\n",
    "    if os.path.exists(full_path):\n",
    "        frame_names = get_frame_names(full_path)\n",
    "        if frame_names:\n",
    "            video_dir = full_path\n",
    "            break\n",
    "\n",
    "if video_dir:\n",
    "    print(f\"使用するフレームディレクトリ: {video_dir}\")\n",
    "    print(f\"フレーム数: {len(frame_names)}\")\n",
    "    print(f\"最初の5フレーム: {frame_names[:5]}\")\n",
    "    \n",
    "    # 最初のフレームを表示\n",
    "    first_frame_path = os.path.join(video_dir, frame_names[0])\n",
    "    image = Image.open(first_frame_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"最初のフレーム\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"画像サイズ: {image.size}\")\n",
    "else:\n",
    "    print(\"フレームディレクトリが見つかりません。\")\n",
    "    print(\"まず動画をフレーム分割してください:\")\n",
    "    print(\"python scripts/video_to_frames.py <動画ファイル> input/dog_images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的な物体検出\n",
    "\n",
    "1フレームで物体検出を試してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if video_dir:\n",
    "    # SAM2トラッカーを初期化\n",
    "    tracker = SAM2VideoTracker(model_size=\"tiny\", device=\"cpu\")\n",
    "    \n",
    "    # 動画を初期化\n",
    "    frame_names = tracker.initialize_video(video_dir)\n",
    "    \n",
    "    # 検出したい座標を指定（画像の中央付近）\n",
    "    image = Image.open(os.path.join(video_dir, frame_names[0]))\n",
    "    center_x, center_y = image.size[0] // 2, image.size[1] // 2\n",
    "    \n",
    "    points = [[center_x, center_y]]  # 中央の座標\n",
    "    labels = [1]  # Positive（検出対象）\n",
    "    \n",
    "    print(f\"検出座標: {points}\")\n",
    "    \n",
    "    # 物体を追加\n",
    "    frame_idx, obj_ids, mask_logits = tracker.add_object_points(\n",
    "        frame_idx=0,\n",
    "        obj_id=0,\n",
    "        points=points,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    # 結果を表示\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"SAM2 物体検出結果\")\n",
    "    \n",
    "    # 元画像を表示\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # 指定座標を表示\n",
    "    points_array = np.array(points, dtype=np.float32)\n",
    "    labels_array = np.array(labels, dtype=np.int32)\n",
    "    show_points(points_array, labels_array, plt.gca())\n",
    "    \n",
    "    # セグメンテーションマスクを表示\n",
    "    mask = (mask_logits[0] > 0.0).cpu().numpy()\n",
    "    show_mask(mask, plt.gca(), obj_id=obj_ids[0])\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ 物体検出完了\")\n",
    "else:\n",
    "    print(\"❌ フレームディレクトリが見つかりません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完全な動画追跡\n",
    "\n",
    "全フレームに対して物体追跡を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if video_dir:\n",
    "    print(\"動画全体での物体追跡を開始...\")\n",
    "    \n",
    "    # 動画全体に追跡を伝播\n",
    "    video_segments = tracker.propagate_in_video()\n",
    "    \n",
    "    print(f\"追跡完了: {len(video_segments)}フレーム処理\")\n",
    "    \n",
    "    # いくつかのフレームの結果を表示\n",
    "    sample_frames = [0, len(frame_names)//4, len(frame_names)//2, 3*len(frame_names)//4, -1]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(sample_frames), figsize=(20, 4))\n",
    "    \n",
    "    for i, frame_idx in enumerate(sample_frames):\n",
    "        if frame_idx == -1:\n",
    "            frame_idx = len(frame_names) - 1\n",
    "            \n",
    "        # フレーム画像を読み込み\n",
    "        image_path = os.path.join(video_dir, frame_names[frame_idx])\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Frame {frame_idx}\")\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # 最初のフレームには座標点も表示\n",
    "        if frame_idx == 0:\n",
    "            show_points(points_array, labels_array, axes[i])\n",
    "        \n",
    "        # マスクを表示\n",
    "        if frame_idx in video_segments:\n",
    "            for obj_id, mask in video_segments[frame_idx].items():\n",
    "                show_mask(mask, axes[i], obj_id=obj_id)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ 動画追跡完了\")\n",
    "else:\n",
    "    print(\"❌ フレームディレクトリが見つかりません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の分析\n",
    "\n",
    "追跡結果を分析し、統計情報を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if video_dir and 'video_segments' in locals():\n",
    "    # 結果を分析\n",
    "    analysis = tracker.analyze_results(video_segments, frame_names)\n",
    "    \n",
    "    print(\"=== 追跡結果分析 ===\")\n",
    "    print(f\"総フレーム数: {analysis['total_frames']}\")\n",
    "    print(f\"処理済みフレーム数: {analysis['processed_frames']}\")\n",
    "    print(f\"検出されたオブジェクト数: {len(analysis['objects_detected'])}\")\n",
    "    \n",
    "    for obj_id, count in analysis['objects_detected'].items():\n",
    "        print(f\"  オブジェクト {obj_id}: {count}フレームで検出\")\n",
    "    \n",
    "    # マスクカバレッジの統計\n",
    "    if analysis['mask_coverage']:\n",
    "        all_coverages = []\n",
    "        for frame_coverages in analysis['mask_coverage'].values():\n",
    "            for coverage in frame_coverages.values():\n",
    "                all_coverages.append(coverage)\n",
    "        \n",
    "        if all_coverages:\n",
    "            print(f\"\\n=== マスクカバレッジ統計 ===\")\n",
    "            print(f\"平均カバレッジ: {np.mean(all_coverages):.2f}%\")\n",
    "            print(f\"最小カバレッジ: {np.min(all_coverages):.2f}%\")\n",
    "            print(f\"最大カバレッジ: {np.max(all_coverages):.2f}%\")\n",
    "            print(f\"標準偏差: {np.std(all_coverages):.2f}%\")\n",
    "            \n",
    "            # カバレッジの推移をプロット\n",
    "            frame_indices = []\n",
    "            coverages = []\n",
    "            \n",
    "            for frame_idx in sorted(analysis['mask_coverage'].keys()):\n",
    "                frame_coverages = analysis['mask_coverage'][frame_idx]\n",
    "                if frame_coverages:\n",
    "                    frame_indices.append(frame_idx)\n",
    "                    coverages.append(list(frame_coverages.values())[0])  # 最初のオブジェクトのみ\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(frame_indices, coverages, 'b-', linewidth=2)\n",
    "            plt.title(\"フレーム別マスクカバレッジの推移\")\n",
    "            plt.xlabel(\"フレーム番号\")\n",
    "            plt.ylabel(\"カバレッジ (%)\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    print(\"\\n✅ 分析完了\")\n",
    "else:\n",
    "    print(\"❌ 分析するデータがありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の保存\n",
    "\n",
    "追跡結果を画像ファイルとして保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if video_dir and 'video_segments' in locals():\n",
    "    # 結果保存ディレクトリ\n",
    "    output_dir = os.path.join(project_root, \"result\", \"notebook_results\")\n",
    "    \n",
    "    print(f\"結果を保存中: {output_dir}\")\n",
    "    \n",
    "    # 最初の10フレームのみ保存（デモ用）\n",
    "    sample_count = min(10, len(frame_names))\n",
    "    \n",
    "    tracker.save_results(\n",
    "        video_dir=video_dir,\n",
    "        frame_names=frame_names[:sample_count],\n",
    "        video_segments={k: v for k, v in video_segments.items() if k < sample_count},\n",
    "        output_dir=output_dir,\n",
    "        show_initial_points=points_array,\n",
    "        show_initial_labels=labels_array\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ {sample_count}フレームを保存完了\")\n",
    "    print(f\"保存先: {output_dir}\")\n",
    "    \n",
    "    # 保存されたファイルを確認\n",
    "    if os.path.exists(output_dir):\n",
    "        saved_files = os.listdir(output_dir)\n",
    "        print(f\"保存されたファイル数: {len(saved_files)}\")\n",
    "        if saved_files:\n",
    "            print(f\"例: {saved_files[:3]}\")\n",
    "else:\n",
    "    print(\"❌ 保存するデータがありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このチュートリアルでは以下を学習しました：\n",
    "\n",
    "1. **SAM2の基本的な使用方法**\n",
    "2. **座標指定による物体検出**\n",
    "3. **動画全体での物体追跡**\n",
    "4. **結果の分析と可視化**\n",
    "5. **結果の保存**\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- 異なる座標での物体検出を試す\n",
    "- 複数のオブジェクトを同時追跡する\n",
    "- より高精度なモデル（large）を試す\n",
    "- GPU環境での高速処理を試す\n",
    "\n",
    "### 参考資料\n",
    "\n",
    "- [SAM2公式リポジトリ](https://github.com/facebookresearch/sam2)\n",
    "- [元記事: SAM2動画内物体検出・追跡](https://qiita.com/Neckoh/items/xxx)\n",
    "- [プロジェクトREADME](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}